# Caption quality analysis

## Objective 

The objective of this experiment is to assess the quality of AI generated captions produced by the system under different prompt configurations

The focus is on qualitative assessment rather than quantitative metrics, reflecting real world usage where perceived quality and relevance matter most 

## Evaluation Criteria
Captions were evaluated using the following criteria 
- Relevance to the image 
- Clarity and readability 
- Appropriateness for social media platforms 
- Alignment with selected tone or vibe

Each criterion was assessed through manual review 

## Experimentation setup

A small set of representative images was selected to cover common use cases such as lifestyle, nature and everyday objects 
For each image
- Multiple prompts were tested 
- Outputs were reviewed side by side
- Observations were recorded manually 

# Observations 
- Structured prompts consistently produced more relevant captions
- Tone guidance significantly improved engagement and natural language flow
- Minimal prompts often resulted in generic or repetitive captions

## Limitations 
- Evaluation is subjective and based on Manual judgement 
- Sample size is limited 
- Results may vary across different models or updates

## Summary 
The experiment demonstrated that prompt structure and tone guidance play a critical role in caption quality. Qualitative evaluation proved effective for identifying meaning improvements in user facing content 
